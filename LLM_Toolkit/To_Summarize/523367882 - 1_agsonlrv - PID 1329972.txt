 why it's, oh god, this is at like 20% volume. How's that? Oh god, I'm still really loud. Hello? I am still really loud and I am at the bottom, but I do want this to be recorded, so I guess I have to work on trying to talk quietly. Can I put this farther away and still pick up? Yeah, let's put it down here. Unreasonably loud, given that we are at the bottom of both volumes, both for the mic and the whole system. But this is what we got. Okay. First of all, let's talk about the ISIS form. So you should have received already by email from the Center for Educational Excellence, I think is what it's called these days, a form for feedback on this course. Until I have actually handed in grades, and in fact, until about a month after that, I cannot see the contents, but I can see how many people have filled it out. And so if by the end of finals, when I'm doing final grade assigning, enough people, a reasonable percentage of the class have filled out the form, I will give the entire class 10 points of value. I will also announce this on Discord when it happens. I haven't decided. Certainly if you hit like 70 or so percent, but I might choose something smaller depending. I kind of got to play it by ear. I guess what this means is, as long as people aren't terrible, I would expect this to happen, and as I said, I will announce it. What would make me happy is 70, so it should be very doable if it gets shared around, and I will share on Piazza and Discord as well. So that then brings a total possible extra credit to 100. Form is out now for CS225. I have not got the stuff together for 173. Next semester, Professor Solomon will be leading 225 and is absolutely looking for people. If you want to be on core staff, we need you. As anyone using the various office hours know, we get incredibly backed up, even with, I think this time, we ended up with like 60 people. So a very large for a paid staff. 124 has a much larger staff, but they don't pay them, so they can have a larger staff. Also, frankly, while it is not easy, we have a lot more students that are ready to support students at 124, whereas a huge part of what being on 225 staff is, is what I know many of you have encountered, is helping people debug MPs, and that is just frankly harder. It is even harder than taking the course, but I do encourage you all who feel you could to join. It's a good way to get to know more of the faculty, since in these huge courses you do not interact with people that much, but in core staff you get a bit more interaction. And a lot of what we do as a course is developed by the staff. And that leads directly into the first thing I sort of want to talk about, which is C++. This course is in C++. It is not about C++. But we do expect you to have learned more about memory management out templates as abstraction. So at a high level, the reason I choose C++ and the reason the people before me had chosen C++ for this course was these two things, was we felt that templates as an abstraction mechanism of separating the kind of data from the kind of usage from the data structure exactly matched how we wanted you to think about data structures, and we thought that when thinking about data structures, one of the fundamental things that matters is thinking about how you're allocating and managing memory. Both of these tasks are forefronted in C++, but exist, frankly, in other languages. So we like C++, and we've used C++ to bring these to the forefront, and why memory management and templates have been a part of almost every single MP and almost every single assignment we've done. That we have always had some set of tests, that tests, are you actually handling memory correctly? And we have almost always used templates to abstract the kind of data we're storing in our structure from the structure itself, which I think are very nice matches. So that's sort of the high-level thought. There were two C++ ideas, or actually there were three C++ ideas that I suspect you may not have really encountered or thought deeply about before coming into the class. The first is the one that we've echoed a lot, which is iterators. That we use iterators consistently in this class and we use, force you to write iterators and use code using iterators. We think as an abstraction to access a data structure, it's an important idea. We want to have this idea walk away with you, though from the point of view of the finals, probably not, that's awesome, not backwards, code on iterator. So we did have you do this before. I think it is actually one of the hardest programming assignments we asked you to do on an exam. I would not expect us to, on the final, repeat that given that we have tested this fairly intensely already. Use iterators, absolutely. The idea that you would be given a data structure where the way to access it is an iterator, absolutely. Absolutely, that you'd be given a data structure where how to access it without changing it is an iterator. Things like that I would expect would be possible, but I would not expect you to have to code an iterator. I would think that we might ask you something conceptual about iterators. So what do we remember? We remember they have to have the pre-increment, the access, and not equals, and that they have to in the class have begin and end. And the thing to remember about end is end is after last. That to me is the thing that catches people the most but it is an idea that makes sense and follows what C coding loop standards look like, what C++ coding loop standards looked like prior to iterators. So it's somewhat unintuitive until you get used to it, but if you just write the code and don't know what an iterator does, it actually ends up looking like you'd expect. But it can catch you out in many cases. So that's what I have to say about iterators. The other two things we talked about were functions as data. and we used functions as data in two different ways. We had functions as data with a lambda function. Define a function in active code, code, and then you can access it. You can either use it immediately or more commonly store it in a variable, probably of type auto, and we defined it with a capture argument, a parameters argument, and then the body. So cap is just a way that we can inline define a function. The only part of this that I think is really funky is the capture. And I think it would be unlikely that we would stress capture, though using it correctly was certainly an important part in KD trees. So I would expect that you could mostly cope with the idea of, for example, making a different comparator, which you probably had to do in MPPuzzles to compare two different things so that your standard priority queue was a min queue instead of a max queue, right? I mean, those are the kinds of things I would expect that you could hopefully do in a test. And functions as pointers, well, again, I'm not going to expect you to be able to generate a function pointer syntax, because the syntax of defining a function pointer is weird. But if you got given a pointer, defined, say, as auto, so you had max of x, y, defined here, some body, all defined, I would expect that you could cope with auto my max equals x. And recognize that I now had a function pointer and that I could use this to call that max function by going my max of 4, 7, for example, and that this would return the value that would be returned by max. So that's all I really expect you to have comfort with is a function pointer. For ECE, this is probably more natural because the idea that there can be a function pointer, well, you know that function code is just stored somewhere in memory, and that all this is is the address of that memory, and the weird bit is, hey, a function's name when we declare it, sorry, when we define it, is the location in memory that that function exists, which again, kind of makes sense, because when the compiler's generating code for calling that function, what it has to do is insert a call to call that function, which is at that location. So this is, again, the more you're comfortable with low-level, the more natural. Mostly I would think that if we gave you some function pointers for something, say hash functions, you could use them. And you could just say, oh, I need to use a hash function. We got a function pointer passed into our thing that was a hash function, or our hash table stored in it a function pointer that was the hash function for this hash table. Those are the kinds of things that I would expect you to do. That's all I've kind of got to say about C++ as an idea to think about. We then go on and talk about lists, and we spent actually a fair amount of time talking about lists, and we said that lists come in two types and we spent time talking about linked or array. I'm still amazed at how loud this is given everything is turned all the way down. And there are advantages and disadvantages. We talked about sorted or unsorted. You implemented a linked list with doubly linked. So I expect that you are comfortable with lists. using them and recognizing their performance. I wouldn't expect you to code a linked list from scratch. Not so much because I think it's impossible, but as we remember from probably most of our experience doing MP lists, things can go off the rails in ways that are really hard to see, and a test is simply not... What I would be testing is have you memorized the code and can you regurgitate something you've memorized, not did you learn something. So I don't think this is, either of these are particularly lucrative, though we would remember that for an array, we like the double to resize. Since that has showed up in other contexts, right? Whenever we ended up using arrays as our data storage, we would recognize that, oh, if we needed to resize, we would be doing doubling. We might have just leaned on standard vector to do that, but sometimes we had specific desires and so would do it ourselves. Okay? So what are the performances of lists? Let's see. Lists, when we start thinking about them, what are the differences, right? Let's see. Look up an arbitrary location. location. Here is where we have an index, right? Index. And here is where we see the advantage because an arbitrary location in an array is constant. That's sort of what defines the array. It's just a mathematical offset. We calculate a location that's the start of the array and go that number of items times the item size over, and we have found it. With a linked list, especially singly, it is order n. Because even if we allowed for our singly linked list to have a tail pointer, or even in fact, actually in this context a doubly linked list, we could always be picking the middle. And we recognize that finding something by index in a linked list is slow. Now, the other hand, when we say given, we mean pointer. Right? So when we say given, you have access to an element. And here, the element is enough because we are talking after the element, but sometimes we'll say given and really mean access to the pointer that points at that thing. Because if we remember how we implemented our list, we used index as a function to go to a place, and then we could do all the operations in constant time. So again, these are here constant, but in our array, since we have to keep the memory contiguous, they're order n. And back here, arbitrary location. Well, with the index, it takes n time to get there and constant time to insert. With the array it takes constant time to get there but n time to insert. So both of these play to the weaknesses. It takes time to find or it takes time to do. And finally search for a value, well, by default in both of these, it's n, but if sorted, so it's n in every case in a singly linked list, but with a array list, if our array is sorted, cool, we can use binary search, and we can do it by jumping to the middle and going forward. Finally, we talked about two specializations of lists, stacks and queues. Stack is, sorry, LIFO, last, in, first, out. right? We build things on top, we can always take off the top. All of these operations are constant. And we can implement a stack with a linked list or with an array list. With a linked list, this is straight constant. With an array list, we end up with amortized constant. Okay, then we have Q, which is first in, first out. This again has constant time operations amortized in the case of the array, but again we can do it in constant time each case. You have to be careful. The obvious solution with an array is to implement a circular queue. So with array, probably circular. I said that a circular array acting as a buffer is an incredibly common data structure. That it's used in many cases to act between multiple processes that are running at potentially different or uneven rates. We just keep something in the middle, and this can be implemented in hardware for incredibly fast performance, but even in software with perfectly reasonable performance. And classically, though this is something that LLMs may actually be changing, this is a tremendous amount of the actual code that gets written in a lot of programs. I have two things that do half my task, and I have to hook them together, and I hook them together by putting a circular queue in the middle. And doing that and getting that right, there's some details, but it's incredibly repetitive work and shows up in all over the place in programming. And most of it is getting types changed or some small amount of filtering work going on. It shows up everywhere. It's something that I do think LLMs and other SDK-based toolkits are likely to help. so after that we spent almost half the course talking about trees trees are so fundamental in computer science honestly as much time as we spent we have only scratched the surface of the things we do with trees trees show up all over the place for all sorts of reasons so So, everybody likes the a tree is an acyclic bell any longer, not that I was good at it ever, trees, which is unusual. That trees with nothing in them is not something that most people talk about. But here we talked about them because we want empty data structures. It's a very common start position, and we have height equal to minus 1. And that becomes an important thing to understand when thinking about how we're going to work with trees and how trees are going to function. We also usually, or let's say expect, rooted and binary. Well, it isn't the case that every tree we looked at was rooted or every tree we looked at was binary. it was by far the most common case that our trees had a direction to them which an acyclic graph does not theoreticians tell you oh well just pick a point make it the root shake it you're done and it's true this isn't insane but when thinking about our trees we talked about them as being rooted because we needed a place to attach them as a data structure and we would attach normally at the root. And for most things we did, we looked at binary because, frankly, it's good enough and it's much less complicated than trees. We talked about tree traversals. Tree traversals find everything in the tree. We said there was in order, pre-order, and post-order, which were all recursive, right? And it was simply a question of do we look at it the first thing we get to the thing, the middle time we get to the thing, or the last time we get to a node. And we defined these fully recursively. These are the most common traversals and these are all kind of analogous to depth-first search, though most classically we would only consider pre-order as analogous to depth-first search because we assume that if we've gotten to some place, we might as well search it. There's no point in waiting to search it later. So So, then we said, hey, this isn't the only way we can go through a tree. We could do, instead, level order, where we want to search everything zero distance from the root, then one distance from the root, then two distance from the root, etc. And we did this, no problem. What is the difference, in case we didn't think stacks or queues were useful? These two algorithms, if written correctly, they're optimizations you can take for one or the other that will break this, but if you write them in the most general way, the only difference between the two algorithms are stacks and queues, and we saw this in MP traversal. And we can also think of level order as BFS. DFS. Okay. Trees is where we really started talking about the idea of dictionaries. And the main trees we looked at for dictionaries were BST, BBST, specifically an AVL tree, and a B tree. And we said, hey, how do these do as a dictionary? And what we said was, well, BST. So we said, hey, if they're okay, it works well because it's bounded by the height, but oops, the height of a binary search tree could be a really bad list, right? It is a perfectly legal tree to only have right children. Why not? Insert into a binary search tree. Nope. Excuse me. Order H. We have to put it in order. Remove. Order H. And all of this is order N. They're still binary trees. We knew how to traverse a binary tree. It doesn't matter that they're search trees, that they are search trees added one property, that a in-order traversal of the tree gave us the data in sorted order, which is pretty cool. And that's true of all of these trees. Balanced binary search tree said, hey, let's be smart how we insert. In the case of an AVL, we did rotations that if we were getting a tree that was unevenly distributed, we could move the values around in the tree, which was the equivalent of imagining they were inserted in a different order, and build a tree that was more balanced, and that led to worst-case behavior of all of these. What was a bee tree? Bee trees got messier, right? Because bee trees have nodes. Since nodes have more than one key, we need the number, either we're a leaf, or we have as many children as we have keys, plus one. So a B tree is an M-ary tree, so that each node can hold at most M-minus one keys. You could define it differently, that was the definition we chose. That's the definition version I expect you to remember, at least for our exam. But, right, what did this do? Did it make things better? Well, the difference here is we got something that had a different base to the log. Because the log of our AVL tree is base two, here we get log base m of n for all of these operations. We get a little bit more complexity that we would have to spend some time about because inserting into the nodes we have this other constant. But from a fundamental value, the advantage here is our trees are very short. So if seeks, so best if following pointer slash edge slow. Our issue here is that we know that if our tree can't fit all in memory, it's either going to be on another machine or on a disk. and these operations accessing a disk or accessing another machine is orders of magnitude slower. A disk is on average right now I think about a million times slower than memory or at least a million times slower than cache. We could have discussions about different levels of cache. And accessing over a network can be even worse, often tens of millions, maybe even hundreds of millions. And so this is millions of instructions. So if in memory we spend a little bit more time to make our out-of-memory operations very few, there's a huge advantage. And honestly, B-trees are used in a whole lot of places, but this idea that if we have something small enough, we can do it very, very fast, but if it gets too big, it becomes slow, is a really important idea. And that when thinking about performance in general, all sorts of performance models are, well, if it's smaller than X, it takes basically no time. But if it's X or more, it takes time. So if we can reduce the number of times we have to access outside of some small number of memory, we can make a huge impact. Then we had special trees. The first special tree we talked about is a KD tree. And the KD tree took advantage of something that was true about our binary search trees, which is range or approximate find. Since a binary search tree is organized the way it is, if we try to find something and it's not in the tree, the nearest thing will either be its pre-order successor or its in-order predecessor or its in-order successor, right? One of those two values will be nearest. But that works in one dimensions. In K dimensions, we said we could build a KD tree by at every level of the tree splitting our data evenly in one dimension, and as we go down, we split in a different dimension at every time. Okay. Pretty cool. We did MP mosaics on that. KD trees are complicated. Certainly wouldn't expect anyone to write a KD tree on an exam, but I do expect you to think and know about why KD trees were structured the way they were, what their advantages were, and the fact that balanced binary search trees of any sort have this approximate find property, that we can either find the nearest value or the value between two ranges efficiently. Then we had Huffman trees. And we said, hey, if we build a tree by putting the least frequently used things at the leaves, that tree, if structured as a binary tree the right way, represents the most efficient way to encode that data. So if I know the frequency of characters in your data, either predictively, because I know that the English language has a certain frequency of characters and I'm encoding English, or explicitly by reading the thing I'm going to encode, I can build a Huffman tree that allows me to encode that data in less bits, where the encoding is the path to the leaf that contains a value. So, compression. Heap, we're going to close because, of course, we are. Okay. The last single tree we saw that was a specialized tree was a heap. And a heap was a data structure that implemented a priority queue. And at this point, I expect you have used these a fair amount. We've had a number of things where you needed to use them to do things efficiently. The heap is interesting for a couple of reasons. One, it's fairly simple, right? If we stick something in, we stick it at the bottom, and then heapify up until we find the right place. If we remove something, we only remove the top, we swap, and heapify down. Because the structure of the heap, we know, will always be a tree like this that we can store as an array. So heaps are stored usually as an array, though not necessarily. and the traditional one is a min-heap, though many of you discovered that the standard template library helpfully implements a max-heap as its default, despite every theoretician I've ever met saying a min-heap is the default. Why? I don't know. So, storing a tree as an array, which leads us to our last tree thing. which were disjoint sets, which was no longer a single tree, but a forest of trees with the ability to add sets, union sets, and find set. Add sets, added sets. I'm not going to... Union sets is took two sets, sets, made one. And it did that by taking the two trees, the up trees, and having one point at the other. And we said that we could do smart union where we would either add the one with less nodes to the one with more nodes, the one with less height to the one with more height, or the one that kind of conceptually had the height. Because we remember that the other thing we do is when we do find and ask what presentative element compression. that when we find something for efficiency, we said, hey, when we do find, we are writing a recursive function that proceeds up the tree until it gets to the top, because this is an upwards-facing tree. And when we get to the top, we know what we would like everything to point to, so as we return from our recurrence, we can move everything to point up to that top unit. Okay. And then finally we did the analysis, and we said, basically, order one, actual order, inverse Ackermann. I'm certainly not going to, since we have said, think of this as constant, I'm not going to have a, ooh, you got it wrong, didn't remember inverse Ackermann. but I think it's worth knowing because you will look better talking to a theoretician if you say disjoint sets is amortized over union and find operations is inverse Ackerman than disjoint sets is amortized constant. Though since the inverse Ackerman function for the number of molecules in the universe we think is about five, who cares? It doesn't matter. But if we had a universe with an infinite number of molecules, maybe we'd care. It's still very, very fast. Okay. Graphs. I'm going to go even faster. Graphs, in a sense, we've already touched pieces on because we said with graphs, we did lots of things. We said there are three representations of graphs. An edge list, an adjacency matrix, and an adjacency list. This one's great. Plus N. This one is not great. And this one is great. Actually, since I am running out of time, we're going to skip forward. Here they are. What should we say about them instead of my writing incoherently? So what did we like? Easy. We liked the edge list because it was super easy. It was very straightforward, and in the context of storing it to a disk, there's no overhead. It's perfect. What did we like about adjacency matrix? We said, well, it's kind of easy to write, and it has one good operation, which is R-adjacent. It has other operations that aren't bad, but it uses a lot of space, and it is usually not a good choice. But it's also kind of easy to write, and if you look around, you see a lot of people do it, because it takes them no time to think about how to write it. Okay. And then we said, well, there's the adjacency list. And this was good in almost every case, but oh my God, there was pointers everywhere, right? It was, I had a pointer to a list of linked lists of pointers to other lists of, it was a big mess. So it was difficult to code, but was essentially optimal in everything but our adjacent. And in fact, in the way we actually use things for algorithms, it's great because it's basically never adding overhead. It is always working in a way that matches what almost every actual algorithm does. So, cool. DFS. In a sense, we already said the difference between the graph and the tree one is don't visit again. In BFS, we have an optimization that's easy to make, but breaks it from being DFS. In BFS, we know that the first time we put a thing into the work queue is the first time we're going to see it. So we can store visiting something, not when we visit it, but the first time we see it. Because we know when we get around to visiting it, it will be that time. This seems great, and your BFS code works great, and then you switch your queue to a stack, and everything is terrible and nothing's matching and it drives you nuts because that optimization doesn't work in DFS. In DFS, you have to put work into the list that you may see before you get to that example. Otherwise, you're not doing DFS. So, FIFO, LIFO. We saw four graph algorithms, arguably we saw three graph algorithms, one of which we renamed and slightly changed. So first we saw Kruskal's and Primm's. these are minimum spanning trees these things had the advantage of letting us find the minimum weight because we now were talking about graphs that had weights on edges set of edges that allowed us to reach everything in the graph they had to be connected for it to work but otherwise it was great This one says, add cheapest edge, and by incredibly short, what are we doing here? We maintain a union find data structure and a priority queue of all the edges by weights. We pull out the smallest edge. If it connects to previously unconnected sets, we use it and connect the sets. And we do this until we find n-1 edges, and we're done. Because at that point, we have connected everything. Because we know a spanning tree will have n-1 edges. Prim says, start somewhere, add cheapest edge to anywhere else. So here we start with a region, one node at first, and then we say, hey, what's the cheapest edge that I can connect to a new node that I haven't seen? And I keep doing that until, again, I have n minus 1 edges, or in fact, until I have reached every edge. The running times of these are order n plus m log n, and order n log n plus m log n. Which looked way different, but when we analyzed it in either a sparse graph or a dense graph, it turned out they're kind of all the same. So which one should you use? I don't have a take-home, other than the fact that Prims leads us into probably the most classic Dijkstra's algorithm, which finds the shortest path from one point to all points in the graph, right? And the key with Dijkstra's algorithm is it's just like Prims, but we include the cost to have got to the point whose edge we're going to reach a new point from. That's it. That's the only change. Prims, but use cost to point plus cost edge. Cool. So since it's exactly like Prims with just a slightly different cost function, the analysis is identical, and we didn't even run it again. We just said, hey, the analysis was exactly identical. And our last algorithm we covered formally was Floyd Warshall. And Floyd Warshall said, hey, instead of finding the shortest path from one point to all points, it's shortest path all, we said it's a dynamic programming algorithm them because it finds first the shortest paths just to the neighbors, then the neighbors plus one, then the neighbors plus another point, and it had the beauty of having the easiest code to analyze the whole time of it's just a triply nested loop. We have zero-minute screen stuff on what we just did, the hashing and probability, and And that'll go up probably on Wednesday, probably on Tuesday. Wednesday, I'm doing question and answers. So I'm gonna be here, I'm not gonna turn the mic on, especially not if it's this loud. And I'll answer questions, I will take them off Discord, I will probably produce like a box where people can write on paper or something, so you can ask them anonymously, but I won't be mic'd, so if you wanna hear, please come. Have a nice day! Alright, let's see, I'm just testing out this mic, I was told it was insanely loud, loud, but it sounds okay to me. Are you guys good with this volume? It's fine, right? Okay. I mean, in his defense, the volume up here is turned to the minimum, so this is the quietest I can get it.