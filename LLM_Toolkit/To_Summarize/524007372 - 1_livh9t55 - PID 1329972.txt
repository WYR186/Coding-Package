 Okay, I know I said I was just going to do questions and answers, but I also said I was going to find time to record the remainder of what I did on Monday, and I have not found time. So I'm going to finish up sort of the high-level walking through the material and have that recorded. Then I will do questions and answers, as I said. So, hashing. Hashing is really cool, but has downsides, right? What are the things that it needs? It needs a good hash function. Specifically, for our purposes, we say it needs to be suha. It needs to match the simple uniform hashing assumption. And we said that the way you can do this is by selecting a hash function from a random family of hash functions that covers all possible hash functions with uniform distribution. And that's really the only way you get the probabilistic. All we're going to expect you to remember on the test is remember that SUHA, if the hash function is SUHA, the chance that any particular value hashes to any particular location is exactly one over the number of locations. And that's really the fundamental trick that we use to do all our analysis. I also said that in a practical sense, that often isn't true, and that's why sometimes hash functions and hash tables actually perform terribly. They usually perform really well, usually great, but sometimes have problems. Then we said, look, we need storage space, so an array to store. And then we need to handle the fact that since the universe of our keys in anything which we're going to use a hash table is larger than the universe of slots, that we can have collisions. And we said, to handle collisions, we either did open or separate chaining. Chaining. God, that's almost unreadable, but I'm going to leave it. Or we did closed. and that was either linear probing or double hashing or something else where the key idea was that in closed we stored everything inside the table and if we saw a collision we went some place farther down. Either the next empty space, or we would skip a, we'd use a separate hash function or some other method to determine how many spaces we skipped. Okay. The other thing is with closed hashing, we needed to have a way to mark whether the thing had been inserted or not, and a way to tell if anything had ever been inserted. So we needed to be able to tell if the slot was empty, and we needed to tell separately if the slot was ever empty, because when checking for something, we might have to check past a slot that is now empty till we got to the slot, because it had been full, till we got to the slot where the thing we are looking for is stored. So what does all of this lead to? It leads to expected O1. Insert and find. All leads to linear space for linear items, so that's great. Expected O1, and that's expected amortized. Now, the thing that is nice, we get actual O1 in separate chaining, right? Because we're just, for insert, we just put it at the front of the list. So even if there's a bunch of things there, it's easy to add it, but removing it and finding it is hard. Okay? Worst case, N. AVL was, if we remember, log n, n, and a linked list, sorted, gets, oh, sorry, gets, for this This one we want to choose unsorted. I put different ones. We get ON and O1. So, right, if we only are putting stuff but rarely look for it, maybe we can use a linked list. It has some advantage. If we did an array, as we saw earlier, Here, we had find becomes O log N if our array is sorted, but insert is then O N. So we go from sort of the worst to the best. But the problem with a hash table is it's not always the best. And sometimes it's going to be very slow when it has to do with rehashing. and sometimes it's going to be very slow because your hash function turned out to be bad so what I want you to take away and I'm not in a sense going to test this because it's too hard to express but this is the thing that personally I've been struggling the most with in teaching this course of content presentation which is hash tables are really useful But usually for a very specific problem, there's a better solution. So you should think of the hash table as the, I don't have a good solution, let's put this here now until I can spend time. Because usually, not always, but usually, there's something better. There's some way that I know things about the data and can use that to organize my data more efficiently. If not, hash tables are pretty good, but sometimes they'll cause you problems. Okay. Continuing, this will go pretty quickly. I almost could have fit it in. We had bloom filters, right? And a bloom filter is kind of just a hash table where instead of storing all of the data, we just stored zero or one, right? Alright, hash table, but don't store the data. So we said that you could do it with one function or possibly k1, 2, k, and hashes. and we said that we got this behavior where the performance, the expected correct answer, the expected errors look like this. So this is as number of hash functions and size. Right, as K goes up and as N goes up. So there is the perfect spot. But that takes work to find, and it isn't important. The other thing, or it isn't, it isn't always practical. The other thing we want to, yeah, I don't have it, okay. We want to talk about with Bloom Filters is they have this table of things that can happen. Well, if it's inserted, yes, a true positive, we have stored something in a bloom filter and the bloom filter says yes, that can happen. We have stored something in a bloom filter and a bloom filter says no. That can't happen, right? If we put something in a bloom filter, we will always get yes. but what can happen is we have not stored something in a bloom filter but the bloom filter says yes and that's our false positive and what can happen is if we have not stored something in our bloom filter and our bloom filter says no which is the correct answer so when we think about algorithms still use bloom filters, we need to remember that of the square here, we can only get three of the four possible outcomes. And this means that as long as we're okay with sometimes saying yes when the correct answer is no, this can be very efficient. It uses very little data and it's very fast. And it's not probabilistically fast, right? It takes a constant amount of time to store something in a bloom filter, and it takes a constant amount of time to look something up in a bloom filter. So it is very fast. So for example, checking if something is in the cache, it'll always say yes if it is in the cache. So then we can look and say, oh, but then sometimes we'll get a cache miss anyways. That's not that big a deal. Similarly, have we been here before? Or should we put this in the cache? Again, you have the same properties. What would be bad is, will this thing explode if I touch it? That would be a terrible choice because sometimes, or that one's okay, sorry. Because it would say, yes, sometimes when it'll explode, and yes, sometimes when it won't explode. But is this safe to touch would be horrible. So we need to think about this. Okay, bloom filters more specifically, right, a probabilistic data structure built from a vector of M and M, all constant. Or, and then the problem is delete is not possible and resize, not possible. Not possible. but they're a neat feature. They're sometimes useful. Yeah, so as the number of things you've put in, it goes up as the number of, right, the more things in it, or sorry, how did I have it? The false positive rate and the number, sorry. I wrote it wrong is why. False positive. Sorry. Thank you, good question. it is also true that the rates go up as n goes up and k but this is the false positive rate so i was thinking of a multi-dimensional graph which i was trying to draw in one dimension so forgive me good call okay then we headed to cardinality which was the last thing we covered We said cardinality is the case where we want to make estimates of how many unique things there are. We can look at all the things, right, but we can only look at them once. So if we were just counting how many things there were, going through all the things would allow us to count it. But we're counting how many unique things there are. And to tell if something is unique, I have to check, have I seen it before, right? And to do that, I would have to store all the things. But we're looking at sets that are 60 billion to 130 trillion items. We cannot store all the things. It would take years to go through checking everything. And so instead, what we stored was the minimum hash value, right? And we said, look, we get this result that the expected value allows us to compute n. That if the smallest value we saw was 95 and the range was 0 to 99, we'd have 1,000. Divided by n plus 1 gets us to n is 9.5. We then said, I left it off, but really change this 0 to 1 instead of 0 to 99, because we can get that by just hashing everything, right? We hash everything, we get a number, we divide that number by the maximum possible hash value, which we know, and we get a number between 0 and 1. And from that, we can compute the value. Specifically, if we store the k smallest, k smallest, we get k divided by n plus 1. So, specifically, we can get cardinality, How many? Unique. We can also get similarity. Compare two sets with union and intersection. and we can compute the similarity by just looking at the elements in our k smallest items and using that to estimate the similarity. So we store k items and we can predict with reasonable degree of accuracy. We didn't do the analysis of the accuracy. It's really probability that's beyond our scope and, frankly, is even more speciously leaning on the assumption that hash functions are suha. So we could do that. And that's it. That was the last thing we talked about. So what is the final? Well, all auto-graded, so you'll find out as soon as it's done. um expect three coding questions i technically haven't finalized the final yet we are which is part of why i didn't get to record this yesterday we were still trying to make sure we were happy about the work we were doing on all the questions um so it is possible i will end up with only two coding questions but i don't think so. This will be slightly over half the points, and that's going to be true whether or not it's three coding questions. I've said it will have, one, will be recursion on trees. Right, the question that we pulled from exam three, because the documentation was missing for some students, if only it was missing for everybody. I don't think you actually needed the documentation, but I'm not willing to have something where half got one experience and half got another. That's been rectified by the coding. The exam has a link to all of the documentation as a question that's worth zero points. So even if we leave it off any particular question, it is available to all of you at all times. But so it'll be something recursive on trees. Build a tree by doing BST inserts, by doing some sort of splitting, compute the heights of a tree, the number of leaves on the tree, create a mirror of a tree, given a tree, you know, all of the things we've done. And as I've said to some people on Monday, I recognize this isn't trivial because people struggle with it, but honestly, it is so important that at this point you have internalized the idea of being able to think recursively on these things. And if you can, these questions are very short. They're exactly following the pattern of do the recursion. We're not going to have anything where you have to figure out something really complicated. for example, on lab trees, there was the compute, is this balanced, right? And that's a little tricky. That requires some thought. The things you're going to ask are, do something where you have to do something recursively, not where you have to come up with a fancy algorithm or a non-obvious algorithm to build it. You'll have to read and understand what we're asking you to do, but it will be fundamentally straightforward as long as you're comfortable with recursion. Then, two others. I've said not linked list. I think linked list code fundamentally, either it goes perfectly or it goes terribly. If it goes terribly, there's no way you can debug it in the exam. It's just not a good test of what you can do. I have also said not graph. Because we just did a graph coding question, I think that was a fairly hard question, and these are supposed to be relatively easy. So what things might I do? Things I would think would be reasonable was possibly implement a single rotation out of an AVL tree where you have otherwise an AVL tree, implement some operations, some of the operations on a heap, implement some of the operations on a hash table, implement a bloom filter if we give you the hash functions. You know, these things should be very straightforward. In fact, they're so straightforward, I would probably then have a show that you can use it in a very straightforward case, right? Oh, now you've fixed our incomplete heap. Run an algorithm that uses a heap. Now you've fixed our hash table. Make a count of how many words happen in this thing, or you can just throw them in the hash table. Again, these should be straightforward. They should be things that you can do without a lot of thought, but show that you can work with C++ fairly easily and fairly well. Because I would hope, with the amount of coding you have done this semester and last semester, that you are fairly comfortable, even if you're coming from ECE, that the amount of programming we've done in general should be fairly straightforward. I'm not going to ask you to do something that requires inheritance or anything that we haven't done a lot of. What else? Up here, the only topic that I guarantee everyone will see is... Oops. How did... I don't even know what that did. Okay. Multiple choice. Choice. Short answer, absolutely have some sketching. What is sketching? Sketching is Bloom filters, cardinalities, and Jaccard estimation. There will absolutely be some number of questions on that. After that, well, the whole semester is up for grabs. But at some level, there are going to be question pools that different people are going to get different topics. For example, I might have a question pool that's pulling from all of the stuff from the first third of class, and everybody gets three questions from that. But that includes a lot of different things. Which ones do you get? I don't know. So, you really should review it all, but these aren't going to be the hard free response questions. There's no free response questions. These aren't going to be super deep, but these are going to be the things we've asked repeatedly, but selected from the entire semester. How do you study? Well, you have six practice exams. You have a bunch of practice exams for the whole semester that pretty much are what we will be pulling from. Will we pull questions from previous exams? Possibly. Will we pull questions that are like previous exams that we've not used before for you? Possibly. So will everybody see every topic? No. I think that for hitting time, I probably don't want that many questions. I don't want this to be overwhelming in time. Will everybody see a selection of all the topics? Yes. Yeah. I have not. Part of that's going to depend on exactly our estimate of how hard the coding is. So my intention is this should take most of you most of the time, but I don't really want it to be... Look, if you're regularly tight on time in exams, you'll probably be tight on time here. but I don't want it to be any worse and that's going to require that once we have finalized exactly what the coding is we will finalize how many multiple choice. Percentage wise the coding will be worth more than the multiple choice you'll be able to tell as soon as you open the exam but exactly where it'll be it would be the most the coding would be worth I think would be 60% of the exam and the least even if something strange happens it would be worth, would be 40% of the exam. For example, if we decide that the set of questions we wanted to use as a second one were way too hard, we might just have two programming questions, and then it would be possible I would decide that more multiple choice. But as I said, for me it's easy to tune the number of multiple choice, because I can just take sections and collapse them together. Instead of selecting three from this, three from that. Select three from all of these. That's going to be a call that I make when it's absolutely finalized. Which, well, will be before tomorrow but I'm not sure what it'll be beyond that. It will be finalized before anyone takes it. The first exam is tomorrow. First time you can take it. That's the goal. Any specific exam questions, because I will record those. I mean, so, hence why I answered yours. If not, I'm going to take the mic off, I'll answer questions about anything anyone wants, and I will play one last silly thing to inspire you for the final. Let's see if I can also get... Let's do it the hard way. Because I want to show the wonderful video as well. A person that was a grad student with me recorded this, and... Heads up, your final is coming soon, one-third of your entire grade. It's real important, we are here to help you study. So let's go over hashing first, and array. The term is false, but should be O of 1. You should remember SUHA. Uniform distribution But that isn't all you need to have a way to handle collisions When two keys hash into the same spot Linear probing pushes a key to the next open spot Separate chaining makes a list of all your elements If you resize, you must rehash Artems may not go to the same place that they were before And that is caching, let's move on Thanks! Cache! So heaps! The priority queues insert and remove our log and time From root to leaves, a meep is always increasing There are ways to build a heap to one And log and one is over then So make sure you know which of these is clearly best. This joint set also might be on your exam. So let's see. They start out at minus one and merge if they are not in the face set. So carefully. You can A union by height. You can B union by size. If you do it right, your efficiency will be maximized. You should use path compression, it gets you up to constant time. Your tree gets shorter with every single call to find. And just like that, you made a maze, figured out disjoint sets in all your MP7 days. Therefore, we're moving on to graphs. Please answer questions about rated graphs like Can these kids get their own swimming pool? You can use Kruskals and or Prims, it doesn't matter which one They both create a tree with smallest total weight You'd better know the difference between a B and DFS And which implementation will suit your graph the best Adjacent C, matrix, or a list Which one will make your graph run the fastest without any tricks? Which is O, N, plus M Last up if you want the shortest path from S to any node Like a third algorithm is surely the way to go All that you need to get this tree Is to have all your graph edges be weighted positively And that's all I've got Good luck on your exam